{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with one variable\n",
    "\n",
    "Outlining multiple linear regression algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "| Value | Meaning |\n",
    "| :-: | :-- |\n",
    "| $m$ | number of training examples |\n",
    "| $x$'s | \"input\" variable / features (ex size of house) |\n",
    "| $y$'s | \"output\" variable / \"target\" variable (ex price of house) |\n",
    "| $(x, y)$ | a single training example (row in table) |\n",
    "| $(x^{(i)}, y^{(i)})$ | $i$th training example (*1 indexed*) |\n",
    "| $(x^{(i)}, y^{(i)});i=1,...,m$ | training set |\n",
    "| $h$ | hypothesis - function that maps from $x$ to $y$ |\n",
    "| $:=$ | assignment operator |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [linear regression model](#Linear-regression-model) - model to solve supervised problems\n",
    "  - [training set](#Training-set) - the input data\n",
    "  - [linear hypothesis](#Linear-hypothesis) - standard hypothesis for linear models\n",
    "- [cost function](#Cost-function) - function to estimate error cost\n",
    "- [gradient descent](#Gradient-descent) - finds parameters for function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model\n",
    "\n",
    "The linear regression model solves [regression problems](1_introduction.ipynb#Regression-problem).\n",
    "\n",
    "![](./static/model_representation_overview.png)\n",
    "\n",
    "Given a [training set](#Training-set), we need a function where $h(x)$ is a good predictor of the corresponding value of $y$. That function is the hypothesis. For linear models we use the [linear hypothesis](#Linear-hypothesis).\n",
    "\n",
    "For example, we could use:\n",
    "\n",
    "- [linear hypothesis](#Linear-hypothesis) for calculating best-fit in linear data\n",
    "- [cost function](#Cost-function) to calculate difference between linear hypothesis and data given a set of parameters\n",
    "- [gradient descent](#Gradient-descent) to find best parameters for the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set\n",
    "\n",
    "The set of input data. One example would be size and price of houses in Vancouver:\n",
    "\n",
    "| Size in feet$^{2}$ ($x$) | Price in 1000's ($y$) |\n",
    "| :-- | :-- |\n",
    "| 2104 | 1024 |\n",
    "| 1416 | 860 |\n",
    "| 1534 | 945 |\n",
    "| 852 | 560 |\n",
    "\n",
    "$$\n",
    "m = 4 \\text{ (4 rows)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear hypothesis\n",
    "\n",
    "The linear hypothesis is used for finding a straight line through linear data.\n",
    "\n",
    "The equation:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x\n",
    "$$\n",
    "\n",
    "Sometimes we just use $h(x)$ for short.\n",
    "\n",
    "![](./static/linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "The cost function is the most commonly used function for [regression problem](1_introduction.ipynb#Regression-problem). It is designed to find the error between the hypothesis and the provided data.\n",
    "\n",
    "> It is sometimes called \"Squared error function\", or \"Mean squared error\".\n",
    "\n",
    "**Hypothesis:** We could use the [linear hypothesis](#Linear-hypothesis).\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "$\\theta_0$ - y intercept at $x = 0$  \n",
    "$\\theta_1$ - slope\n",
    "\n",
    "The cost function can be defined as:\n",
    "\n",
    "$$\n",
    "J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "In the cost function, $h_{\\theta}(x^{(i)})$ is the same as $\\theta_0 + \\theta_1x^{(i)}$ (note this is the same as the hypothesis, and it is effected by changing $\\theta$'s).\n",
    "\n",
    "It is equal to $\\frac{1}{2}\\bar{x}$ where $\\bar{x}$ is the mean of the squares of $h_\\theta(x^{(i)}) - y^{(i)}$. This equals the difference between predicted and actual value. It is halved for convenience of computing [gradient descent](#Gradient-descent) (derivative of the square will cancel out the half).\n",
    "\n",
    "We want to choose $\\theta_0$ and $\\theta_1$ such that $h_{\\theta}(x)$ is close to $y$ for our training examples $(x, y)$ (minimizing error).\n",
    "\n",
    "$\\substack{minimize\\\\\\theta_0\\theta_1} J(\\theta_0, \\theta_1)$\n",
    "\n",
    "$\\substack{minimize\\\\\\theta_0\\theta_1}$ means \"find me the values of $\\theta_0$ and $\\theta_1$ that minimize the equation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "> Sometimes called \"Batch gradient descent\" (looks at all training set - some versions may not).\n",
    "\n",
    "Gradient descent is a way of estimating the best parameters for a function. Depending on the function, gradient descent may converge on local optima (a local \"low point\") rather than the global optima. The steps are:\n",
    "\n",
    "1. choose an arbitrary starting value for each parameter\n",
    "2. \"step downhill\" (towards lowest proximal function value)\n",
    "3. continue until you reach a global minimum value for your function\n",
    "\n",
    "> There is a normal equations method which will solve the same problems without multiple steps, but it doesn't scale as well with large training sets\n",
    "\n",
    "There can be an arbitrary number of $\\theta$'s, but the following graph shows just two: $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "![](./static/gradient_descent_graph.png)\n",
    "\n",
    "Repeat until convergence (for $j = 0$ and $j = 1$ - aka do it for both thetas):\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1)\n",
    "$$\n",
    "\n",
    "$\\alpha$ = **learning rate** - controls how big of a step we take \"downhill\". If too small, might take too long to reach minimum. If too large, could overshoot and even fail to converge.\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1)$ = **derivative term** - the current slope of the function $J$ given the current $\\theta_0$ and $\\theta_1$. This allows for the \"steps\" to get smaller as we approach the global minimum (where slope will be 0).\n",
    "\n",
    "\n",
    "All thetas must be updated simultaneously:\n",
    "\n",
    "```go\n",
    "temp0 := gradientDescent(theta0)\n",
    "temp1 := gradientDescent(theta1)\n",
    "theta0, theta1 := temp0, temp1\n",
    "```\n",
    "\n",
    "### Example with zero-value for $\\theta_0$\n",
    "\n",
    "Simplify by setting $\\theta_0 = 0$ (same as removing $\\theta_0$ from the equations).\n",
    "\n",
    "Gradient descent:\n",
    "\n",
    "$$\n",
    "\\theta_1 := \\theta_1 - \\alpha\\frac{\\partial}{\\partial\\theta_1}J(\\theta_1)\n",
    "$$\n",
    "\n",
    "![](./static/gradient_descent_example.png)\n",
    "\n",
    "Derivative term has positive slope, so $\\theta_1 := \\theta_1 - \\alpha(positive)$ means $\\theta_1$ will decrease (moving towards the minimum).\n",
    "\n",
    "### Example using the cost function\n",
    "\n",
    "Plug in the [cost function](#Cost-function):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1) = \\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "We need to determine for $j=0,1$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "j &= 0:\\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\\\\n",
    "j &= 1:\\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Repeat until convergence:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_0 &:= \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\theta_1 &:= \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
