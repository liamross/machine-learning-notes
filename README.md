# Machine learning notes

> ðŸš§ This is a work in progress!

Notes written while taking the [Stanford machine learning course](https://www.coursera.org/learn/machine-learning).

Notes taken in Jupyter notebook, so it's recommended to view in [Jupyter Notebook Viewer](https://nbviewer.jupyter.org/github/liamross/machine-learning-notes/tree/master/) (any Jupyter links below will open within Jupyter Notebook Viewer).

## Modules

These follow the progression of the course and include notes taken while watching the videos (links will open into Jupyter Notebook Viewer).

<!-- modules:startnum -->
1. [Introduction](https://nbviewer.jupyter.org/github/liamross/machine-learning-notes/blob/master/modules/1_introduction.ipynb) - basic terminology for machine learning
1. [Linear regression with one variable](https://nbviewer.jupyter.org/github/liamross/machine-learning-notes/blob/master/modules/2_linear_regression_with_one_variable.ipynb) - simple regression algorithms
1. [Linear algebra review](https://nbviewer.jupyter.org/github/liamross/machine-learning-notes/blob/master/modules/3_linear_algebra_review.ipynb) - an optional refresher on matrices and vectors
1. [Linear regression with multiple variables](https://nbviewer.jupyter.org/github/liamross/machine-learning-notes/blob/master/modules/4_linear_regression_with_multiple_variables.ipynb) - more complete regression algorithms
1. [Octave tutorial](https://nbviewer.jupyter.org/github/liamross/machine-learning-notes/blob/master/modules/5_octave_tutorial.ipynb) - commands and tips for vectorizing in Octave
1. [Logistic regression](https://nbviewer.jupyter.org/github/liamross/machine-learning-notes/blob/master/modules/6_logistic_regression.ipynb)
<!-- modules:endnum -->

## Notes

These are more structured notes that combine information under topics, potentially spanning multiple modules (links will open into Jupyter Notebook Viewer).

<!-- notes:start -->
- [Cost function](https://nbviewer.jupyter.org/github/liamross/machine-learning-notes/blob/master/notes/cost_function.ipynb) - function for calculating average error from
- [Gradient descent](https://nbviewer.jupyter.org/github/liamross/machine-learning-notes/blob/master/notes/gradient_descent.ipynb) - algorithm for reducing hypothesis error
- [Linear hypothesis](https://nbviewer.jupyter.org/github/liamross/machine-learning-notes/blob/master/notes/linear_hypothesis.ipynb) - hypothesis for linear data
- [Notation](https://nbviewer.jupyter.org/github/liamross/machine-learning-notes/blob/master/notes/notation.ipynb) - a reference for notation used throughout these notes
<!-- notes:end -->

## Octave examples

The folder `octave_examples` will contain some example Octave functions for certain algorithms covered in the course. These should also run in Matlab, as the two are generally compatible.

<!-- octave_examples:start -->
- [costFunction](https://github.com/liamross/machine-learning-notes/blob/master/octave_examples/costFunction.m) - cost function implementation for any number of variables
- [gradientDescent](https://github.com/liamross/machine-learning-notes/blob/master/octave_examples/gradientDescent.m) - gradient descent implementation with additional cost function history
- [meanStandardNormalize](https://github.com/liamross/machine-learning-notes/blob/master/octave_examples/meanStandardNormalize.m) - a feature normalization technique
- [normalEquation](https://github.com/liamross/machine-learning-notes/blob/master/octave_examples/normalEquation.m) - normal equation implementation
<!-- octave_examples:end -->

## Assignments

I have git ignored the `assignments` folder since it contains answers to the programming assignments. I highly encourage doing [the course](https://www.coursera.org/learn/machine-learning) yourself. It's free!

## Other information

All illustrations done using [Excalidraw](https://excalidraw.com/).