{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Multiple features](#Multiple-features)\n",
    "- [Multivariate linear regression](#Multivariate-linear-regression)\n",
    "- [Gradient descent for multiple variables](#Gradient-descent-for-multiple-variables)\n",
    "  - [Feature scaling](#Feature-scaling)\n",
    "    - [Mean normalization](#Mean-normalization)\n",
    "  - [Learning rate](#Learning-rate)\n",
    "- [Features and polynomial regression](#Features-and-polynomial-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple features\n",
    "\n",
    "We use $x_n$ to denote an input variable (and $y$ to denote output).\n",
    "\n",
    "For example, we have the training set:\n",
    "\n",
    "| Size ($\\text{feet}^2$) ($x_1$) | # of bedrooms ($x_2$) | # of floors ($x_3$) | Age (years) ($x_4$) | Price (x\\\\$1000) ($y$) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 2104 | 5 | 1 | 45 | 460 |\n",
    "| 1416 | 3 | 2 | 40 | 232 |\n",
    "| 1534 | 3 | 2 | 30 | 315 |\n",
    "| 852 | 2 | 1 | 36 | 178 |\n",
    "\n",
    "$x^{(2)}$ is the 2-index item in the training set:\n",
    "\n",
    "$$\n",
    "x^{(2)} = \\begin{bmatrix}1416 \\\\ 3 \\\\ 2 \\\\ 40\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$x_3^{(2)}$ is the 3-index item in the $x^{(2)}$ vector:\n",
    "\n",
    "$$\n",
    "x_3^{(2)} = 2\n",
    "$$\n",
    "\n",
    "A multi-feature hypothesis takes the form of assigning a weight to each variable, and a base value:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 + \\theta_4x_4\n",
    "$$\n",
    "\n",
    "For example, perhaps some good theta values would be:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = 40 + 0.9x_1 + 10x_2 + 4x_3 - 1.1x_4\n",
    "$$\n",
    "\n",
    "Essentially this says the base price of a house is **\\\\$40,000**, and there is a **\\\\$900** per square foot, an extra **\\\\$10,000** per bedroom, an extra **\\\\$4,000** per floor, and finally a reduction of **\\\\$1,100** per year of age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression\n",
    "\n",
    "This is the hypothesis used when doing linear regression with multiple variables. The basic form is this:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n\n",
    "$$\n",
    "\n",
    "Simplify by assuming $x_0 = 1$ ($x_0^{(i)} = 1$):\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n \\quad\n",
    "x = \\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{bmatrix} \\in \\mathbb{R}^{n+1} \\quad\n",
    "\\theta = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ ... \\\\ \\theta_n\\end{bmatrix} \\in \\mathbb{R}^{n+1}\n",
    "$$\n",
    "\n",
    "Hypothesis is now equal to:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^Tx\n",
    "$$\n",
    "\n",
    "This is equal to the previous form since:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) =\n",
    "\\theta^Tx =\n",
    "\\begin{bmatrix}\\theta_0 & \\theta_1 & \\theta_2 & ... & \\theta_n\\end{bmatrix} \\times \\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{bmatrix} =\n",
    "\\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n \\quad(x_0 = 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent for multiple variables\n",
    "\n",
    "Hypothesis: $h_\\theta(x) = \\theta^Tx$\n",
    "\n",
    "Parameters: $\\theta$ (the n+1 dimensional vector)\n",
    "\n",
    "Cost function: $J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "Equation (done simultaneously for every $j=0,...,n$):\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n",
    "$$\n",
    "\n",
    "New algorithm (with the partial derivative of cost function - remember that $x_0^{(i)} = 1$):\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\n",
    "$$\n",
    "\n",
    "As before, this must be repeated simultaneously until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "\n",
    "Make sure that features are on a similar scale (this will cause gradient descent to converge more quickly). By making sure scale is similar, the contours will be more circular leading to a quicker convergence.\n",
    "\n",
    "The basic equation where $S_i$ = range (max - min) or standard deviation:\n",
    "\n",
    "$$\n",
    "\\frac{x_i}{S_i}\n",
    "$$\n",
    "\n",
    "For example, if we have an $x_1$ = size (0-2000 feet$^2$) and $x_2$ = # of bedrooms (0-5):\n",
    "\n",
    "$$\n",
    "x_1 = \\frac{\\text{size (feet$^2$)}}{2000} \\quad x_2 = \\frac{\\text{number of bedrooms}}{5}\n",
    "$$\n",
    "\n",
    "Goal: get every feature to approximately a $-1 \\leq x_i \\leq 1$ range.\n",
    "\n",
    "> Andrew Ng's rule of thumb:\n",
    ">\n",
    "> $-3 \\leq x_i \\leq 3$ is max\n",
    ">\n",
    "> $-\\frac{1}{3} \\leq x_i \\leq \\frac{1}{3}$ is min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean normalization\n",
    "\n",
    "Rather than standard feature scaling, you instead replace $x_i$ with $x_i - \\mu_i$ (features will have zero mean - this does not apply to $x_0$).\n",
    "\n",
    "For example, if the mean size is 1000 and mean number of bedrooms is 2:\n",
    "\n",
    "$$\n",
    "x_1 = \\frac{\\text{size - 1000}}{2000} \\quad x_2 = \\frac{\\text{bedrooms - 2}}{5}\n",
    "$$\n",
    "\n",
    "Values will now fall roughly between $-0.5 \\leq x_i \\leq 0.5$ range.\n",
    "\n",
    "Now the equation where $\\mu_i$ = average value of $x_i$ in training set is:\n",
    "\n",
    "$$\n",
    "\\frac{x_i - \\mu_i}{S_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "This focuses on the learning rate $\\alpha$ in the gradient descent update rule:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n",
    "$$\n",
    "\n",
    "First, you can plot the changing value of $J(\\theta)$ with number of iterations:\n",
    "\n",
    "![](./static/min_over_number_iterations.png)\n",
    "\n",
    "Some points regarding the data:\n",
    "\n",
    "- $J(\\theta)$ should decrease after **every** iteration. This may depend on using a sufficiently small $\\alpha$ value\n",
    "- It is hard to determine in advance how many iterations it will take to converge\n",
    "\n",
    "In order to determine whether you have converged, you could create a convergence test such as declaring a number like $10^{-3}$ where if $J(\\theta)$ decreases by less in a single iteration you are complete. However, choosing the number can be quite difficult, so graphing like above can be more effective.\n",
    "\n",
    "When choosing $\\alpha$, try $..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, ...$ (3x increases)\n",
    "\n",
    "- $\\alpha$ too small = very slow convergence, non-optimal\n",
    "- $\\alpha$ too large = may not converge (may also have slow convergence due to bouncing)\n",
    "\n",
    "![](./static/various_learning_rates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and polynomial regression\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
