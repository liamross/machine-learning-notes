{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Multiple features](#Multiple-features) - how to handle multiple features in linear regression\n",
    "- [Multivariate linear regression](#Multivariate-linear-regression) - hypothesis for multiple variables\n",
    "- [Gradient descent for multiple variables](#Gradient-descent-for-multiple-variables) - general equation for gradient descent\n",
    "  - [Feature scaling](#Feature-scaling) - divide by range to scale features correctly\n",
    "    - [Mean normalization](#Mean-normalization) - in addition to feature scaling, subtract mean\n",
    "  - [Learning rate](#Learning-rate) - optimizing the rate for gradient descent\n",
    "- [Choosing features](#Choosing-features) - how to pick correct features and create new ones\n",
    "- [Polynomial regression](#Polynomial-regression) - fit curves rather than linear\n",
    "- [Normal equation](#Normal-equation) - solve better than gradient descent in some cases\n",
    "  - [Noninvertibility](#Noninvertibility) - what happens when normal equation is non-invertible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple features\n",
    "\n",
    "We use $x_n$ to denote an input variable (and $y$ to denote output).\n",
    "\n",
    "For example, we have the training set:\n",
    "\n",
    "| Size ($\\text{feet}^2$) ($x_1$) | # of bedrooms ($x_2$) | # of floors ($x_3$) | Age (years) ($x_4$) | Price (x\\\\$1000) ($y$) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 2104 | 5 | 1 | 45 | 460 |\n",
    "| 1416 | 3 | 2 | 40 | 232 |\n",
    "| 1534 | 3 | 2 | 30 | 315 |\n",
    "| 852 | 2 | 1 | 36 | 178 |\n",
    "\n",
    "$x^{(2)}$ is the 2-index item in the training set:\n",
    "\n",
    "$$\n",
    "x^{(2)} = \\begin{bmatrix}1416 \\\\ 3 \\\\ 2 \\\\ 40\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$x_3^{(2)}$ is the 3-index item in the $x^{(2)}$ vector:\n",
    "\n",
    "$$\n",
    "x_3^{(2)} = 2\n",
    "$$\n",
    "\n",
    "A multi-feature hypothesis takes the form of assigning a weight to each variable, and a base value:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 + \\theta_4x_4\n",
    "$$\n",
    "\n",
    "For example, perhaps some good theta values would be:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = 40 + 0.9x_1 + 10x_2 + 4x_3 - 1.1x_4\n",
    "$$\n",
    "\n",
    "Essentially this says the base price of a house is **\\\\$40,000**, and there is a **\\\\$900** per square foot, an extra **\\\\$10,000** per bedroom, an extra **\\\\$4,000** per floor, and finally a reduction of **\\\\$1,100** per year of age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression\n",
    "\n",
    "This is the hypothesis used when doing linear regression with multiple variables. The basic form is this:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n\n",
    "$$\n",
    "\n",
    "Simplify by assuming $x_0 = 1$ ($x_0^{(i)} = 1$):\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n \\quad\n",
    "x = \\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{bmatrix} \\in \\mathbb{R}^{n+1} \\quad\n",
    "\\theta = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ ... \\\\ \\theta_n\\end{bmatrix} \\in \\mathbb{R}^{n+1}\n",
    "$$\n",
    "\n",
    "Hypothesis is now equal to:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^Tx\n",
    "$$\n",
    "\n",
    "This is equal to the previous form since:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) =\n",
    "\\theta^Tx =\n",
    "\\begin{bmatrix}\\theta_0 & \\theta_1 & \\theta_2 & ... & \\theta_n\\end{bmatrix} \\times \\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{bmatrix} =\n",
    "\\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n \\quad(x_0 = 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent for multiple variables\n",
    "\n",
    "Hypothesis: $h_\\theta(x) = \\theta^Tx$\n",
    "\n",
    "Parameters: $\\theta$ (the n+1 dimensional vector)\n",
    "\n",
    "Cost function: $J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "Equation (done simultaneously for every $j=0,...,n$):\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n",
    "$$\n",
    "\n",
    "New algorithm (with the partial derivative of cost function - remember that $x_0^{(i)} = 1$):\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\n",
    "$$\n",
    "\n",
    "As before, this must be repeated simultaneously until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "\n",
    "Make sure that features are on a similar scale (this will cause gradient descent to converge more quickly). By making sure scale is similar, the contours will be more circular leading to a quicker convergence.\n",
    "\n",
    "The basic equation where $S_i$ = range (max - min) or standard deviation:\n",
    "\n",
    "$$\n",
    "\\frac{x_i}{S_i}\n",
    "$$\n",
    "\n",
    "For example, if we have an $x_1$ = size (0-2000 feet$^2$) and $x_2$ = # of bedrooms (0-5):\n",
    "\n",
    "$$\n",
    "x_1 = \\frac{\\text{size (feet$^2$)}}{2000} \\quad x_2 = \\frac{\\text{number of bedrooms}}{5}\n",
    "$$\n",
    "\n",
    "Goal: get every feature to approximately a $-1 \\leq x_i \\leq 1$ range.\n",
    "\n",
    "> Andrew Ng's rule of thumb:\n",
    ">\n",
    "> $-3 \\leq x_i \\leq 3$ is max\n",
    ">\n",
    "> $-\\frac{1}{3} \\leq x_i \\leq \\frac{1}{3}$ is min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean normalization\n",
    "\n",
    "Rather than standard feature scaling, you instead replace $x_i$ with $x_i - \\mu_i$ (features will have zero mean - this does not apply to $x_0$).\n",
    "\n",
    "For example, if the mean size is 1000 and mean number of bedrooms is 2:\n",
    "\n",
    "$$\n",
    "x_1 = \\frac{\\text{size - 1000}}{2000} \\quad x_2 = \\frac{\\text{bedrooms - 2}}{5}\n",
    "$$\n",
    "\n",
    "Values will now fall roughly between $-0.5 \\leq x_i \\leq 0.5$ range.\n",
    "\n",
    "Now the equation where $\\mu_i$ = average value of $x_i$ in training set is:\n",
    "\n",
    "$$\n",
    "\\frac{x_i - \\mu_i}{S_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "This focuses on the learning rate $\\alpha$ in the gradient descent update rule:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n",
    "$$\n",
    "\n",
    "First, you can plot the changing value of $J(\\theta)$ with number of iterations:\n",
    "\n",
    "![](./static/min_over_number_iterations.png)\n",
    "\n",
    "Some points regarding the data:\n",
    "\n",
    "- $J(\\theta)$ should decrease after **every** iteration. This may depend on using a sufficiently small $\\alpha$ value\n",
    "- It is hard to determine in advance how many iterations it will take to converge\n",
    "\n",
    "In order to determine whether you have converged, you could create a convergence test such as declaring a number like $10^{-3}$ where if $J(\\theta)$ decreases by less in a single iteration you are complete. However, choosing the number can be quite difficult, so graphing like above can be more effective.\n",
    "\n",
    "When choosing $\\alpha$, try $..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, ...$ (3x increases)\n",
    "\n",
    "- $\\alpha$ too small = very slow convergence, non-optimal\n",
    "- $\\alpha$ too large = may not converge (may also have slow convergence due to bouncing)\n",
    "\n",
    "![](./static/various_learning_rates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing features\n",
    "\n",
    "Choosing your features may result in a better model.\n",
    "\n",
    "Choosing your features is an important step. For example, when calculating housing price let's say you are given a **frontage** (width of lot) and a **depth**. Rather than calculating using frontage as $x_1$ and depth as $x_2$, you may choose to **create your own features** by combining them into **area**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "\n",
    "Given a non-linear set of data, you may choose something like a quadratic model ($\\theta_0 + \\theta_1x + \\theta_2x^2$) or, in the case of housing prices rising with size, perhaps a cubic function ($\\theta_0 + \\theta_1x + \\theta_2x^2 + \\theta_3x^3$) since quadratic models are parabolic.\n",
    "\n",
    "![](./static/polynomial_regression.png)\n",
    "\n",
    "So given the multivariate linear regression hypothesis, how do we map it to the cubic function:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 = \\theta_0 + \\theta_1x + \\theta_2x^2 + \\theta_3x^3\n",
    "$$\n",
    "\n",
    "Essentially, you can configure each x to be size to a different power:\n",
    "\n",
    "$$\n",
    "x_1 = (size) \\quad x_2 = (size)^2 \\quad x_3 = (size)^3\n",
    "$$\n",
    "\n",
    "If you choose your features like this, the linear regression model maps directly to the polynomial function, and you have a **cubic fit** to the data.\n",
    "\n",
    "Note that feature scaling is now a concern! For houses of size range 800-1000 square feet (200 range):\n",
    "\n",
    "$$\n",
    "x_1 = \\frac{(size)}{200} \\quad x_2 = \\frac{(size)^2}{200^2} \\quad x_3 = \\frac{(size)^3}{200^3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equation\n",
    "\n",
    "Better than gradient descent for **some linear regression problems**.\n",
    "\n",
    "Pros:\n",
    "\n",
    "1. Done in a single step (no need for iterations)\n",
    "1. Does not require feature scaling\n",
    "1. Does not require a learning rate $\\alpha$\n",
    "\n",
    "Cons:\n",
    "\n",
    "1. Slow if number of features $n$ is large ($X^TX$ is an $n \\times n$ matrix, cost to compute is roughly $O(n^3)$)\n",
    "1. Doesn't work for some classification problem algorithms\n",
    "\n",
    "> Begin considering gradient descent somewhere around $n=10000$\n",
    "\n",
    "Basically for the vector $\\theta$ we want to set the derivative (slope) of the cost function to 0 for every value $j$.\n",
    "\n",
    "Cost function: $J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "Goal: $\\frac{\\partial}{\\partial\\theta_j}J(\\theta) = ... = 0$\n",
    "\n",
    "Solving the derivatives is quite involved, so instead here's an example of how to solve it.\n",
    "\n",
    "The **normal equation** is defined as:\n",
    "\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "Here's an example of how to find X and y. In this example, we are solving for housing prices and we have $m=4$:\n",
    "\n",
    "| ($x_0$) | Size in feet$^2$ ($x_1$) | # bedrooms ($x_2$) | # floors ($x_3$) | Age in years ($x_4$) | Price (x1000) ($y$) |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 1 | 2104 | 5 | 1 | 45 | 460 |\n",
    "| 1 | 1416 | 3 | 2 | 40 | 232 |\n",
    "| 1 | 1534 | 3 | 2 | 30 | 315 |\n",
    "| 1 | 852 | 2 | 1 | 36 | 178 |\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & 2104 & 5 & 1 & 45 \\\\\n",
    "1 & 1416 & 3 & 2 & 40 \\\\\n",
    "1 & 1534 & 3 & 2 & 30 \\\\\n",
    "1 & 852 & 2 & 1 & 36 \\\\\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "y = \\begin{bmatrix}\n",
    "460 \\\\\n",
    "232 \\\\\n",
    "315 \\\\\n",
    "178 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> To create X, each row is $(x^{(i)})^T, i = 0,...,m$\n",
    "\n",
    "X is an $m\\times(n+1)$ dimensional matrix and y is an $m$-dimensional vector.\n",
    "\n",
    "Finally we calculate the normal equation $\\theta = (X^TX)^{-1}X^Ty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.09353825e-05],\n",
       "       [ 2.12425850e-01],\n",
       "       [ 2.27277367e+01],\n",
       "       [ 1.07168045e-01],\n",
       "       [-6.53624164e+01],\n",
       "       [-5.79337497e+00]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "X = np.array([\n",
    "    [1, 2104, 5, 2104, 1, 45],\n",
    "    [1, 1416, 3, 2104, 2, 40],\n",
    "    [1, 1534, 3, 2104, 2, 30],\n",
    "    [1, 852, 2, 2104, 1, 36],\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [460],\n",
    "    [232],\n",
    "    [315],\n",
    "    [178],\n",
    "])\n",
    "\n",
    "XT = np.transpose(X)\n",
    "\n",
    "pinv(XT @ X) @ XT @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noninvertibility\n",
    "\n",
    "Q: How do we calculate the inverse of $X^TX$ if it is non-invertible (singular/degenerate)?\n",
    "\n",
    "A: `pinv` will do it for you (pseudo-inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.zeros((2, 2))\n",
    "\n",
    "Inverse = pinv(A)\n",
    "Inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: When would it be non-invertible?\n",
    "\n",
    "A: A few causes:\n",
    "\n",
    "1. Redundant features (linearly dependent)\n",
    "  - Example: $x_1$ = size in feet squared and $x_2$ = size in meters squared\n",
    "1. Too many features (example: $m \\leq n$)\n",
    "  - Example: $m=10$ (10 training set items) but $n=100$ (100 features)\n",
    "  - To solve: delete some features or use **regularization**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
