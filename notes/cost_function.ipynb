{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function tests a hypothesis with a set of parameters against the actual target value to determine the \"cost\" or error between the two. This can be used with [gradient descent](gradient_descent.ipynb) to gradually find the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Quick reference](#Quick-reference) - quick reference for equations\n",
    "- [Equation](#Equation) - the cost function\n",
    "- [Vectorized](#Vectorized) - the vectorized equation\n",
    "- [Octave](#Octave) - octave implementation\n",
    "- [Python](#Python) - python implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick reference\n",
    "\n",
    "Basic equation:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "Vectorized equation (with [linear hypothesis](linear_hypothesis.ipynb)):\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} (X\\theta - y)^T(X\\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equation\n",
    "\n",
    "The difference between the hypothesis and the actual value is calculated for each item in the training set, squared, and summed. Finally, it is divided by the number of items in the training set to get the mean error. It is halved for convenience of calculating the derivative in gradient descent:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized\n",
    "\n",
    "To run cost function in a vectorized way you first need to prep a matrix $X$ with a column for each training set. You will also need a vector $y$ with all of the output variables.\n",
    "\n",
    "First, choose your hypothesis. For example, let's use the [linear hypothesis](linear_hypothesis.ipynb):\n",
    "\n",
    "$$\n",
    "X\\theta\n",
    "$$\n",
    "\n",
    "Now calculate the error:\n",
    "\n",
    "$$\n",
    "X\\theta - y\n",
    "$$\n",
    "\n",
    "Now we will multiply the transpose error with itself in order to square and sum the values:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} (X\\theta - y)^T(X\\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Octave\n",
    "\n",
    "> View the code for [costFunction](https://github.com/liamross/machine-learning-notes/blob/master/octave_examples/costFunction.m) with comments here.\n",
    "\n",
    "```octave\n",
    "function J = costFunction (X, y, theta)\n",
    "\n",
    "    m = length(y);\n",
    "    hypotheses = X * theta;\n",
    "    err = hypotheses - y;\n",
    "    J = (1 / (2 * m)) * err' * err;\n",
    "\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(X, y, theta):\n",
    "    m = len(y)\n",
    "    hypothesis = X @ theta\n",
    "    err = hypothesis - y\n",
    "    return ((1 / (2 * m)) * (np.transpose(err) @ err).item((0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it against some fake housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With bad theta values  - cost: 2184583333333.3333\n",
      "With good theta values - cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [1, 3000],\n",
    "    [1, 4000],\n",
    "    [1, 5000],\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [1550000],\n",
    "    [2050000],\n",
    "    [2550000],\n",
    "])\n",
    "\n",
    "bad_theta = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "])\n",
    "\n",
    "good_theta = np.array([\n",
    "    [50000],\n",
    "    [500],\n",
    "])\n",
    "\n",
    "print(\"With bad theta values  - cost:\", costFunction(X, y, bad_theta))\n",
    "\n",
    "print(\"With good theta values - cost:\", costFunction(X, y, good_theta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
